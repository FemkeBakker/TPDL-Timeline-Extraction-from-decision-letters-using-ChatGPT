{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Timeline Extraction Pipeline\n",
    "Our task is to extract timelines from government decision letters using ChatGPT. \n",
    "In this notebook we will evaluate all steps of the algorithm. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date Extraction & Correction\n",
    "Here we will extract and correct the dates. In preprocessing the PDF files have already been converted to txt files. The first step is to split the txt files into sentences, detect the dates and discard sentences that do not have a date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the date correction algorithm: 0.9768339768339769\n",
      "Total dates not correctly extracted: 6 out of 259\n",
      "Original length of dataframe:547\n",
      "New length after removing mistakes: 541\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "from code.scripts.date_correction import compile, accuracy_dates\n",
    "from code.scripts.remove_mistakes import remove_false_dates\n",
    " \n",
    "# select all dates using spacy\n",
    "with open('code/data/results/date_extraction/uncorrected_dates_test.pkl', 'rb') as fp:\n",
    "    test_uncorrected = pickle.load(fp)\n",
    "\n",
    "# correct incomplete dates and filter out nonsense date selected by spacy\n",
    "test_corrected = compile(test_uncorrected)\n",
    "\n",
    "# load ground truth test set\n",
    "gt_test = pd.read_csv(\"code/data/GT/GTtest/all_dates.csv\")\n",
    "gt_test_date_event = gt_test.loc[gt_test['label'] != 'DATE+']\n",
    "\n",
    "# calculate accuracy on test set & return dataset with mistakes removed\n",
    "accuracy_test_dates, mistakes, test_dates = accuracy_dates(gt_test_date_event, test_corrected)\n",
    "\n",
    "print(f\"Accuracy of the date correction algorithm: {accuracy_test_dates}\")\n",
    "print(f\"Total dates not correctly extracted: {mistakes} out of {len(gt_test_date_event)}\")\n",
    "\n",
    "# remove dates with an event that have not been extracted correctly\n",
    "clean_test_dates = remove_false_dates(test_dates, gt_test)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision date extraction\n",
    "Next, we will extract the \"decision made\" dates. Then, we will remove all dates that are in truth decision dates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation metrics values for classifying the decision date on testset: \n",
      " Accuracy: 0.9963031423290203 \n",
      " Recall: 1.0 \n",
      " Precision: 0.96 \n",
      " F1-score: 0.9795918367346939\n",
      "Original length of dataframe:541\n",
      "New length of dataframe after removing mistakes: 493\n"
     ]
    }
   ],
   "source": [
    "from code.scripts.decision import decision_class, evaluate_decision\n",
    "\n",
    "test_decision = decision_class(clean_test_dates)\n",
    "\n",
    "# return evaluation & df with prediction and truth \n",
    "accuracy, recall, precision, f1, values, test_copy = evaluate_decision(test_decision, gt_test)\n",
    "print(f\"Evaluation metrics values for classifying the decision date on testset: \\n Accuracy: {accuracy} \\n Recall: {recall} \\n Precision: {precision} \\n F1-score: {f1}\")\n",
    "\n",
    "# remove all date where decisiondate truth = True. Keep dates that still need to be classified\n",
    "print(f\"Original length of dataframe:{len(test_decision)}\")\n",
    "test_dates_no_decision = test_copy.loc[test_copy['truth'] == False].drop(columns=['truth', \"decisiondate\"])\n",
    "print(f\"New length of dataframe after removing mistakes: {len(test_dates_no_decision)}\")\n",
    "test_dates_no_decision.to_csv(\"code/data/results/chatgpt_extraction/input.csv\", index=False)\n",
    "\n",
    "# select all decision dates that were predicted correctly\n",
    "decision_dates = test_copy.loc[(test_copy['truth'] == True) & (test_copy['decisiondate'] == test_copy['truth'])]\n",
    "\n",
    "# select decision mistakes\n",
    "decision_mistakes = test_copy.loc[(test_copy['decisiondate'] != test_copy['truth'])]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChatGPT 1: event phrase extraction & filtering dates\n",
    "We ran the ChatGPT experiment in code/run_chatGPT.ipynb. The results are loaded and evaluated here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.63      0.51      0.56       288\n",
      "        True       0.45      0.58      0.51       205\n",
      "\n",
      "    accuracy                           0.54       493\n",
      "   macro avg       0.54      0.54      0.53       493\n",
      "weighted avg       0.55      0.54      0.54       493\n",
      "\n",
      "{'fp': 142, 'tp': 118, 'fn': 87, 'tn': 146}\n",
      "Total dates with an event of which ChatGPT extracted an event phrase: 144\n",
      "Average jaccard similarity: 50.184% \n",
      " Fraction of dates that overlap >= 50%: 53.472% \n",
      " Fraction of dates that overlap >= 75%: 27.083% \n",
      " Fraction of dates that overlap = 100%: 8.333% \n"
     ]
    }
   ],
   "source": [
    "from code.scripts.chatgpt_extraction import evaluate\n",
    "\n",
    "# load predictions and event descriptions\n",
    "extraction_predictions = pd.read_csv(\"code/data/results/chatgpt_extraction/predictions.csv\")\n",
    "\n",
    "# load ground truth\n",
    "gt = pd.read_csv(\"code/data/GT/GTtest/date_event_combinations.csv\")\n",
    "\n",
    "# print out evaluation metrics\n",
    "evaluate(extraction_predictions, gt)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChatGPT 2: Event Classification\n",
    "We ran the ChatGPT experiment in code/run_chatGPT.ipynb. The results are loaded and evaluated here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             precision    recall  f1-score   support\n",
      "\n",
      "     beslistermijn verdaagd       0.82      0.86      0.84        21\n",
      "                    contact       0.88      0.70      0.78        40\n",
      "   inwerking treden van Woo       1.00      0.88      0.93        16\n",
      "ontvangst verzoek bevestigd       0.95      0.80      0.86        44\n",
      "                     overig       0.60      0.52      0.56        23\n",
      "              verzoek datum       0.77      1.00      0.87        46\n",
      "          verzoek ontvangen       0.50      0.67      0.57        15\n",
      "\n",
      "                   accuracy                           0.80       205\n",
      "                  macro avg       0.79      0.77      0.77       205\n",
      "               weighted avg       0.81      0.80      0.80       205\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load predictions\n",
    "predictions_classification = pd.read_csv(\"code/data/results/chatgpt_classification/predictions.csv\")\n",
    "\n",
    "from code.scripts.chatgpt_classification import evaluate\n",
    "df_predictions_classification = evaluate(predictions_classification, gt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
